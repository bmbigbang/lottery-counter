{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "labels = [\"Lucky Star 1\", \"Lucky Star 2\", \"Ball 1\", \"Ball 2\", \"Ball 3\", \"Ball 4\", \"Ball 5\"]\n",
    "column_labels = [\"Lucky Star 1\", \"Lucky Star 2\", \"Ball 1\", \"Ball 2\", \"Ball 3\", \"Ball 4\", \"Ball 5\", \"Ball Delta\", \"Ball Y Delta\", \"LS Delta\", \"LS Y Delta\",\n",
    "    \"Ball 1 Repeat Last 5\", \"Ball 2 Repeat Last 5\", \"Ball 3 Repeat Last 5\", \"Ball 4 Repeat Last 5\", \"Ball 5 Repeat Last 5\", \"Lucky Star 1 Repeat Last 5\", \"Lucky Star 2 Repeat Last 5\",\n",
    "    \"Ball 1 Repeat Last 10\", \"Ball 2 Repeat Last 10\", \"Ball 3 Repeat Last 10\", \"Ball 4 Repeat Last 10\", \"Ball 5 Repeat Last 10\", \"Lucky Star 1 Repeat Last 10\", \"Lucky Star 2 Repeat Last 10\",\n",
    "    \"Ball 1 Repeat Last 20\", \"Ball 2 Repeat Last 20\", \"Ball 3 Repeat Last 20\", \"Ball 4 Repeat Last 20\", \"Ball 5 Repeat Last 20\", \"Lucky Star 1 Repeat Last 20\", \"Lucky Star 2 Repeat Last 20\"\n",
    "]\n",
    "column_indices = {name: i for i, name in enumerate(column_labels)}\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"lottery-numbers-tracker.xlsx\", sheet_name=\"EuroMillions\", header=0)\n",
    "\n",
    "df[column_labels].to_csv(\"euromillions-dataset.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:38:09.464186Z",
     "start_time": "2025-02-23T12:38:06.742873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('euromillions-dataset.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:38:09.511906Z",
     "start_time": "2025-02-23T12:38:09.492199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Lucky Star 1  Lucky Star 2  Ball 1  Ball 2  Ball 3  Ball 4  Ball 5  \\\n",
       "0               2            11       1       3      11      20      43   \n",
       "1               5             7       5      14      25      26      40   \n",
       "2               3            10       4      14      31      36      38   \n",
       "3               1             6       3      21      36      41      46   \n",
       "4               8            10      10      23      24      29      45   \n",
       "..            ...           ...     ...     ...     ...     ...     ...   \n",
       "311             6             9       1      19      36      38      49   \n",
       "312             9            10      10      25      29      34      45   \n",
       "313             2             4       6      16      18      39      47   \n",
       "314             7            12       5      14      35      36      39   \n",
       "315             2             5       3      12      19      24      30   \n",
       "\n",
       "     Ball Delta  Ball Y Delta  LS Delta  ...  Ball 5 Repeat Last 10  \\\n",
       "0      6.509608      3.888444         9  ...                      1   \n",
       "1      4.993746      2.374868         2  ...                      0   \n",
       "2      5.111262      2.561250         7  ...                      0   \n",
       "3      6.118619      3.698648         5  ...                      0   \n",
       "4      5.309190      4.643275         2  ...                      0   \n",
       "..          ...           ...       ...  ...                    ...   \n",
       "311    6.791539      2.814249         3  ...                      0   \n",
       "312    4.918079      3.143247         1  ...                      0   \n",
       "313    6.169481      3.831449         2  ...                      0   \n",
       "314    5.766281      4.422669         5  ...                      0   \n",
       "315    3.455069      8.921883         3  ...                      0   \n",
       "\n",
       "     Lucky Star 1 Repeat Last 10  Lucky Star 2 Repeat Last 10  \\\n",
       "0                              1                            0   \n",
       "1                              3                            1   \n",
       "2                              1                            2   \n",
       "3                              1                            2   \n",
       "4                              2                            1   \n",
       "..                           ...                          ...   \n",
       "311                            0                            1   \n",
       "312                            0                            0   \n",
       "313                            1                            0   \n",
       "314                            0                            0   \n",
       "315                            0                            0   \n",
       "\n",
       "     Ball 1 Repeat Last 20  Ball 2 Repeat Last 20  Ball 3 Repeat Last 20  \\\n",
       "0                        2                      4                      1   \n",
       "1                        1                      3                      2   \n",
       "2                        2                      3                      0   \n",
       "3                        3                      2                      1   \n",
       "4                        2                      2                      4   \n",
       "..                     ...                    ...                    ...   \n",
       "311                      0                      1                      1   \n",
       "312                      0                      0                      0   \n",
       "313                      0                      0                      0   \n",
       "314                      0                      0                      0   \n",
       "315                      0                      0                      0   \n",
       "\n",
       "     Ball 4 Repeat Last 20  Ball 5 Repeat Last 20  \\\n",
       "0                        2                      1   \n",
       "1                        1                      0   \n",
       "2                        2                      0   \n",
       "3                        5                      1   \n",
       "4                        6                      1   \n",
       "..                     ...                    ...   \n",
       "311                      0                      0   \n",
       "312                      0                      0   \n",
       "313                      1                      0   \n",
       "314                      0                      0   \n",
       "315                      0                      0   \n",
       "\n",
       "     Lucky Star 1 Repeat Last 20  Lucky Star 2 Repeat Last 20  \n",
       "0                              2                            1  \n",
       "1                              4                            3  \n",
       "2                              1                            4  \n",
       "3                              2                            5  \n",
       "4                              5                            3  \n",
       "..                           ...                          ...  \n",
       "311                            0                            1  \n",
       "312                            0                            0  \n",
       "313                            1                            0  \n",
       "314                            0                            0  \n",
       "315                            0                            0  \n",
       "\n",
       "[316 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lucky Star 1</th>\n",
       "      <th>Lucky Star 2</th>\n",
       "      <th>Ball 1</th>\n",
       "      <th>Ball 2</th>\n",
       "      <th>Ball 3</th>\n",
       "      <th>Ball 4</th>\n",
       "      <th>Ball 5</th>\n",
       "      <th>Ball Delta</th>\n",
       "      <th>Ball Y Delta</th>\n",
       "      <th>LS Delta</th>\n",
       "      <th>...</th>\n",
       "      <th>Ball 5 Repeat Last 10</th>\n",
       "      <th>Lucky Star 1 Repeat Last 10</th>\n",
       "      <th>Lucky Star 2 Repeat Last 10</th>\n",
       "      <th>Ball 1 Repeat Last 20</th>\n",
       "      <th>Ball 2 Repeat Last 20</th>\n",
       "      <th>Ball 3 Repeat Last 20</th>\n",
       "      <th>Ball 4 Repeat Last 20</th>\n",
       "      <th>Ball 5 Repeat Last 20</th>\n",
       "      <th>Lucky Star 1 Repeat Last 20</th>\n",
       "      <th>Lucky Star 2 Repeat Last 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>6.509608</td>\n",
       "      <td>3.888444</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>4.993746</td>\n",
       "      <td>2.374868</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>5.111262</td>\n",
       "      <td>2.561250</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "      <td>6.118619</td>\n",
       "      <td>3.698648</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>5.309190</td>\n",
       "      <td>4.643275</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>49</td>\n",
       "      <td>6.791539</td>\n",
       "      <td>2.814249</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>4.918079</td>\n",
       "      <td>3.143247</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "      <td>6.169481</td>\n",
       "      <td>3.831449</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "      <td>5.766281</td>\n",
       "      <td>4.422669</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>3.455069</td>\n",
       "      <td>8.921883</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows Ã— 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "\n",
    "def create_sequences(data, seq_length, pred_cols):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)].values[::-1]\n",
    "        y = data.iloc[i + seq_length, :pred_cols].values\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# --- 2. Load and Preprocess Data ---\n",
    "\n",
    "# Parameters\n",
    "seq_length = 7  # Number of past rows to look at\n",
    "pred_cols = 7  # Number of columns to predict\n",
    "all_cols = 32\n",
    "random_seed_length = 7 # Length of random seed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:38:26.959599Z",
     "start_time": "2025-02-23T12:38:26.221651Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:38:29.064430Z",
     "start_time": "2025-02-23T12:38:29.056783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_values = np.concatenate(([12, 12, 50, 50, 50, 50, 50], np.zeros(all_cols - pred_cols)))\n",
    "min_values = np.concatenate(([1, 1, 1, 1, 1, 1, 1], np.zeros(all_cols - pred_cols)))\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = df[::-1]\n",
    "scaled_df = pd.concat([scaled_df, pd.DataFrame([max_values], columns=df.columns)], ignore_index=True)\n",
    "scaled_df = pd.concat([scaled_df, pd.DataFrame([min_values], columns=df.columns)], ignore_index=True)\n",
    "scaled_df.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:38:30.588706Z",
     "start_time": "2025-02-23T12:38:30.486145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale the data\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(scaled_df), columns=df.columns)\n",
    "scaled_df = scaled_df.iloc[:-2]\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(scaled_df, seq_length, pred_cols)\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Input for the sequence\n",
    "sequence_input = tf.keras.layers.Input(shape=(seq_length, all_cols))\n",
    "\n",
    "# Input for the random seed\n",
    "random_seed_input = tf.keras.layers.Input(shape=(random_seed_length,))\n",
    "\n",
    "scaled_df.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:38:34.054744Z",
     "start_time": "2025-02-23T12:38:34.049880Z"
    }
   },
   "cell_type": "code",
   "source": "scaler.inverse_transform([np.concatenate((y[-1], np.zeros(all_cols - pred_cols)))])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., 11.,  1.,  3., 11., 20., 43.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:38:44.201332Z",
     "start_time": "2025-02-23T12:38:44.195678Z"
    }
   },
   "cell_type": "code",
   "source": "scaler.inverse_transform(X[-1])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.        ,  7.        ,  5.        , 14.        , 25.        ,\n",
       "        26.        , 40.        ,  4.99374609,  2.37486842,  2.        ,\n",
       "         2.54950976,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  1.        ,  0.        ,  2.        ,\n",
       "         0.        ,  0.        ,  0.        ,  3.        ,  1.        ,\n",
       "         1.        ,  3.        ,  2.        ,  1.        ,  0.        ,\n",
       "         4.        ,  3.        ],\n",
       "       [ 3.        , 10.        ,  4.        , 14.        , 31.        ,\n",
       "        36.        , 38.        ,  5.11126208,  2.56124969,  7.        ,\n",
       "         3.16227766,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  2.        ,  1.        ,  1.        ,\n",
       "         0.        ,  2.        ,  0.        ,  1.        ,  2.        ,\n",
       "         2.        ,  3.        ,  0.        ,  2.        ,  0.        ,\n",
       "         1.        ,  4.        ],\n",
       "       [ 1.        ,  6.        ,  3.        , 21.        , 36.        ,\n",
       "        41.        , 46.        ,  6.11861913,  3.6986484 ,  5.        ,\n",
       "         5.70087713,  1.        ,  0.        ,  0.        ,  2.        ,\n",
       "         0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ,  5.        ,  0.        ,  1.        ,  2.        ,\n",
       "         3.        ,  2.        ,  1.        ,  5.        ,  1.        ,\n",
       "         2.        ,  5.        ],\n",
       "       [ 8.        , 10.        , 10.        , 23.        , 24.        ,\n",
       "        29.        , 45.        ,  5.30919015,  4.64327471,  2.        ,\n",
       "         2.12132034,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  1.        ,  1.        ,  0.        ,  0.        ,\n",
       "         1.        ,  3.        ,  0.        ,  2.        ,  1.        ,\n",
       "         2.        ,  2.        ,  4.        ,  6.        ,  1.        ,\n",
       "         5.        ,  3.        ],\n",
       "       [ 5.        , 10.        ,  3.        ,  4.        , 29.        ,\n",
       "        39.        , 43.        ,  6.80991924,  7.04840407,  5.        ,\n",
       "         3.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         2.        ,  0.        ,  0.        ,  2.        ,  0.        ,\n",
       "         2.        ,  1.        ,  5.        ,  1.        ,  1.        ,\n",
       "         4.        ,  2.        ],\n",
       "       [ 2.        ,  7.        , 22.        , 30.        , 41.        ,\n",
       "        44.        , 49.        ,  3.69966215,  2.97993289,  5.        ,\n",
       "         1.58113883,  0.        ,  1.        ,  3.        ,  0.        ,\n",
       "         2.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
       "         4.        ,  0.        ,  2.        ,  0.        ,  1.        ,\n",
       "         1.        ,  1.        ,  4.        ,  1.        ,  2.        ,\n",
       "         1.        ,  2.        ],\n",
       "       [ 1.        ,  9.        ,  9.        , 37.        , 41.        ,\n",
       "        42.        , 49.        ,  7.28868987,  7.35662966,  8.        ,\n",
       "         1.58113883,  0.        ,  1.        ,  2.        ,  2.        ,\n",
       "         1.        ,  0.        ,  1.        ,  0.        ,  2.        ,\n",
       "         3.        ,  2.        ,  1.        ,  1.        ,  2.        ,\n",
       "         0.        ,  2.        ,  3.        ,  3.        ,  2.        ,\n",
       "         1.        ,  6.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:39:04.468386Z",
     "start_time": "2025-02-23T12:39:04.465871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for testing sequences\n",
    "# df = df[::-1]\n",
    "# print(y[-1])\n",
    "# print(df.iloc[-1].values[:7])"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 3. Model Definition ---\n",
    "\n",
    "# LSTM layer\n",
    "lstm1_1 = tf.keras.layers.LSTM(128, return_sequences=False)(sequence_input)\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(128, activation='sigmoid')(lstm1_1)\n",
    "\n",
    "dense_2 = tf.keras.layers.Dense(14, activation='sigmoid')(dense_1)\n",
    "\n",
    "concat = tf.keras.layers.Concatenate()([dense_2, random_seed_input])\n",
    "\n",
    "output = tf.keras.layers.Dense(pred_cols)(concat)\n",
    "\n",
    "# Create the model with two inputs\n",
    "lstm_model = tf.keras.models.Model(inputs=[sequence_input, random_seed_input], outputs=output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T12:39:06.008183Z",
     "start_time": "2025-02-23T12:39:05.754112Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:39:07.703615Z",
     "start_time": "2025-02-23T12:39:07.692001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --- 4. Loss Function with Emphasis ---\n",
    "\n",
    "def custom_loss(y_true, y_pred, random_target):\n",
    "    # Calculate loss\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    weighted_mse_loss = tf.reduce_mean(squared_error)\n",
    "\n",
    "    # MSE between prediction and random target\n",
    "    mse_loss_random = tf.reduce_mean(tf.square(y_pred - random_target))\n",
    "\n",
    "    # Combine the two losses (you can adjust the weights)\n",
    "    return 0.6 * weighted_mse_loss + 0.4 * mse_loss_random\n",
    "\n",
    "def generate_random_target(batch_size):\n",
    "    targets = []\n",
    "    for _ in range(batch_size):\n",
    "        lucky_target = []\n",
    "        target = []\n",
    "        while len(lucky_target) < 2:\n",
    "            num = np.random.randint(1, 13)\n",
    "            if num not in lucky_target:\n",
    "                lucky_target.append(num)\n",
    "        while len(target) < 5:\n",
    "            num = np.random.randint(1, 51)\n",
    "            if num not in target:\n",
    "                target.append(num)\n",
    "        lucky_target = np.array(lucky_target) / 12\n",
    "        target = np.array(target) / 50\n",
    "        targets.append(np.concatenate((lucky_target, target)))\n",
    "    return np.array(targets)\n",
    "\n",
    "# --- 5. Compile the Model ---\n",
    "\n",
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   loss=custom_loss)  # Use the custom weighted loss\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:39:10.647227Z",
     "start_time": "2025-02-23T12:39:10.643841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(X_train.shape)\n",
    "\n",
    "print(scaler.data_min_)\n",
    "\n",
    "print(scaler.data_max_)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278, 7, 32)\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[12.         12.         50.         50.         50.         50.\n",
      " 50.          8.10092587 11.56719499 11.          9.05538514  2.\n",
      "  3.          4.          3.          3.          3.          4.\n",
      "  4.          5.          5.          5.          5.          5.\n",
      "  6.          6.          6.          7.          6.          7.\n",
      "  9.         10.        ]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T12:42:22.607368Z",
     "start_time": "2025-02-23T12:39:18.046284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 6. Train the Model ---\n",
    "\n",
    "# Custom training loop\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Get batch data\n",
    "\n",
    "        X_batch = X_train[i:(i + batch_size)]\n",
    "        y_batch = y_train[i:(i + batch_size)]\n",
    "        random_seeds_batch = generate_random_target(X_batch.shape[0])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get predictions\n",
    "            predictions = lstm_model([X_batch, random_seeds_batch], training=True)\n",
    "\n",
    "            # Combine the two losses (you can adjust the weights)\n",
    "            loss = custom_loss(y_batch, predictions, random_seeds_batch)\n",
    "\n",
    "        # Compute and apply gradients\n",
    "        gradients = tape.gradient(loss, lstm_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, lstm_model.trainable_variables))\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / (X_train.shape[0] // batch_size)}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.506454586982727\n",
      "Epoch 2/300, Loss: 0.17006602883338928\n",
      "Epoch 3/300, Loss: 0.1321173906326294\n",
      "Epoch 4/300, Loss: 0.11522729694843292\n",
      "Epoch 5/300, Loss: 0.10949696600437164\n",
      "Epoch 6/300, Loss: 0.1073889434337616\n",
      "Epoch 7/300, Loss: 0.10905632376670837\n",
      "Epoch 8/300, Loss: 0.09713529050350189\n",
      "Epoch 9/300, Loss: 0.10160938650369644\n",
      "Epoch 10/300, Loss: 0.09866175055503845\n",
      "Epoch 11/300, Loss: 0.10199722647666931\n",
      "Epoch 12/300, Loss: 0.09698591381311417\n",
      "Epoch 13/300, Loss: 0.09384238719940186\n",
      "Epoch 14/300, Loss: 0.09301091730594635\n",
      "Epoch 15/300, Loss: 0.09540598839521408\n",
      "Epoch 16/300, Loss: 0.09302066266536713\n",
      "Epoch 17/300, Loss: 0.08766140788793564\n",
      "Epoch 18/300, Loss: 0.08967989683151245\n",
      "Epoch 19/300, Loss: 0.08745458722114563\n",
      "Epoch 20/300, Loss: 0.08675780147314072\n",
      "Epoch 21/300, Loss: 0.08424884080886841\n",
      "Epoch 22/300, Loss: 0.0842650756239891\n",
      "Epoch 23/300, Loss: 0.07974892109632492\n",
      "Epoch 24/300, Loss: 0.08194193243980408\n",
      "Epoch 25/300, Loss: 0.07939905673265457\n",
      "Epoch 26/300, Loss: 0.07555676251649857\n",
      "Epoch 27/300, Loss: 0.08140945434570312\n",
      "Epoch 28/300, Loss: 0.07890977710485458\n",
      "Epoch 29/300, Loss: 0.07551611214876175\n",
      "Epoch 30/300, Loss: 0.07711195200681686\n",
      "Epoch 31/300, Loss: 0.0738251730799675\n",
      "Epoch 32/300, Loss: 0.0768367350101471\n",
      "Epoch 33/300, Loss: 0.07255183160305023\n",
      "Epoch 34/300, Loss: 0.07394203543663025\n",
      "Epoch 35/300, Loss: 0.07141347229480743\n",
      "Epoch 36/300, Loss: 0.07241753488779068\n",
      "Epoch 37/300, Loss: 0.0719437450170517\n",
      "Epoch 38/300, Loss: 0.07000842690467834\n",
      "Epoch 39/300, Loss: 0.06797058880329132\n",
      "Epoch 40/300, Loss: 0.06844504922628403\n",
      "Epoch 41/300, Loss: 0.0704689770936966\n",
      "Epoch 42/300, Loss: 0.06693845987319946\n",
      "Epoch 43/300, Loss: 0.06969708949327469\n",
      "Epoch 44/300, Loss: 0.06592085212469101\n",
      "Epoch 45/300, Loss: 0.06674682348966599\n",
      "Epoch 46/300, Loss: 0.06459105014801025\n",
      "Epoch 47/300, Loss: 0.06484862416982651\n",
      "Epoch 48/300, Loss: 0.06606684625148773\n",
      "Epoch 49/300, Loss: 0.0642974004149437\n",
      "Epoch 50/300, Loss: 0.06314288079738617\n",
      "Epoch 51/300, Loss: 0.0633237287402153\n",
      "Epoch 52/300, Loss: 0.06272365152835846\n",
      "Epoch 53/300, Loss: 0.06201395392417908\n",
      "Epoch 54/300, Loss: 0.06047879904508591\n",
      "Epoch 55/300, Loss: 0.06113378331065178\n",
      "Epoch 56/300, Loss: 0.060251425951719284\n",
      "Epoch 57/300, Loss: 0.06254688650369644\n",
      "Epoch 58/300, Loss: 0.060833629220724106\n",
      "Epoch 59/300, Loss: 0.06243595480918884\n",
      "Epoch 60/300, Loss: 0.061207447201013565\n",
      "Epoch 61/300, Loss: 0.060826968401670456\n",
      "Epoch 62/300, Loss: 0.061834391206502914\n",
      "Epoch 63/300, Loss: 0.06248489022254944\n",
      "Epoch 64/300, Loss: 0.062249232083559036\n",
      "Epoch 65/300, Loss: 0.05825790390372276\n",
      "Epoch 66/300, Loss: 0.05815764516592026\n",
      "Epoch 67/300, Loss: 0.05915214866399765\n",
      "Epoch 68/300, Loss: 0.05955377593636513\n",
      "Epoch 69/300, Loss: 0.057748597115278244\n",
      "Epoch 70/300, Loss: 0.05879153311252594\n",
      "Epoch 71/300, Loss: 0.05950203537940979\n",
      "Epoch 72/300, Loss: 0.06067236140370369\n",
      "Epoch 73/300, Loss: 0.05656159296631813\n",
      "Epoch 74/300, Loss: 0.06000044196844101\n",
      "Epoch 75/300, Loss: 0.057808104902505875\n",
      "Epoch 76/300, Loss: 0.05906674265861511\n",
      "Epoch 77/300, Loss: 0.05782017484307289\n",
      "Epoch 78/300, Loss: 0.05936373025178909\n",
      "Epoch 79/300, Loss: 0.058530114591121674\n",
      "Epoch 80/300, Loss: 0.05835152417421341\n",
      "Epoch 81/300, Loss: 0.05824807286262512\n",
      "Epoch 82/300, Loss: 0.0562121607363224\n",
      "Epoch 83/300, Loss: 0.0556669719517231\n",
      "Epoch 84/300, Loss: 0.05732943117618561\n",
      "Epoch 85/300, Loss: 0.058544378727674484\n",
      "Epoch 86/300, Loss: 0.05789706110954285\n",
      "Epoch 87/300, Loss: 0.05701083689928055\n",
      "Epoch 88/300, Loss: 0.05826428905129433\n",
      "Epoch 89/300, Loss: 0.05720621719956398\n",
      "Epoch 90/300, Loss: 0.05736760422587395\n",
      "Epoch 91/300, Loss: 0.058006927371025085\n",
      "Epoch 92/300, Loss: 0.05895649641752243\n",
      "Epoch 93/300, Loss: 0.057527054101228714\n",
      "Epoch 94/300, Loss: 0.057313255965709686\n",
      "Epoch 95/300, Loss: 0.05714891105890274\n",
      "Epoch 96/300, Loss: 0.059025440365076065\n",
      "Epoch 97/300, Loss: 0.05559542775154114\n",
      "Epoch 98/300, Loss: 0.05808720365166664\n",
      "Epoch 99/300, Loss: 0.05717087164521217\n",
      "Epoch 100/300, Loss: 0.05863254517316818\n",
      "Epoch 101/300, Loss: 0.05721113085746765\n",
      "Epoch 102/300, Loss: 0.05783478915691376\n",
      "Epoch 103/300, Loss: 0.05695834010839462\n",
      "Epoch 104/300, Loss: 0.05609942600131035\n",
      "Epoch 105/300, Loss: 0.05765755474567413\n",
      "Epoch 106/300, Loss: 0.05705549567937851\n",
      "Epoch 107/300, Loss: 0.0565008670091629\n",
      "Epoch 108/300, Loss: 0.057639747858047485\n",
      "Epoch 109/300, Loss: 0.057043176144361496\n",
      "Epoch 110/300, Loss: 0.056386422365903854\n",
      "Epoch 111/300, Loss: 0.05683121457695961\n",
      "Epoch 112/300, Loss: 0.05556811764836311\n",
      "Epoch 113/300, Loss: 0.05681593343615532\n",
      "Epoch 114/300, Loss: 0.05687704309821129\n",
      "Epoch 115/300, Loss: 0.05812480300664902\n",
      "Epoch 116/300, Loss: 0.05631230026483536\n",
      "Epoch 117/300, Loss: 0.05724393203854561\n",
      "Epoch 118/300, Loss: 0.05599590390920639\n",
      "Epoch 119/300, Loss: 0.057285793125629425\n",
      "Epoch 120/300, Loss: 0.0558074489235878\n",
      "Epoch 121/300, Loss: 0.05702114850282669\n",
      "Epoch 122/300, Loss: 0.056670207530260086\n",
      "Epoch 123/300, Loss: 0.055358268320560455\n",
      "Epoch 124/300, Loss: 0.05617139860987663\n",
      "Epoch 125/300, Loss: 0.05590490251779556\n",
      "Epoch 126/300, Loss: 0.05639708787202835\n",
      "Epoch 127/300, Loss: 0.055104922503232956\n",
      "Epoch 128/300, Loss: 0.0566372387111187\n",
      "Epoch 129/300, Loss: 0.057129330933094025\n",
      "Epoch 130/300, Loss: 0.05628642439842224\n",
      "Epoch 131/300, Loss: 0.05431269109249115\n",
      "Epoch 132/300, Loss: 0.0551031194627285\n",
      "Epoch 133/300, Loss: 0.05617111176252365\n",
      "Epoch 134/300, Loss: 0.05663057044148445\n",
      "Epoch 135/300, Loss: 0.05467996001243591\n",
      "Epoch 136/300, Loss: 0.05515259876847267\n",
      "Epoch 137/300, Loss: 0.05544807389378548\n",
      "Epoch 138/300, Loss: 0.0565124936401844\n",
      "Epoch 139/300, Loss: 0.056448373943567276\n",
      "Epoch 140/300, Loss: 0.05478798225522041\n",
      "Epoch 141/300, Loss: 0.053511012345552444\n",
      "Epoch 142/300, Loss: 0.05619240179657936\n",
      "Epoch 143/300, Loss: 0.05639520660042763\n",
      "Epoch 144/300, Loss: 0.05226505547761917\n",
      "Epoch 145/300, Loss: 0.054350219666957855\n",
      "Epoch 146/300, Loss: 0.05357234552502632\n",
      "Epoch 147/300, Loss: 0.05416647717356682\n",
      "Epoch 148/300, Loss: 0.057476479560136795\n",
      "Epoch 149/300, Loss: 0.0524427630007267\n",
      "Epoch 150/300, Loss: 0.05434325709939003\n",
      "Epoch 151/300, Loss: 0.05578068271279335\n",
      "Epoch 152/300, Loss: 0.05179193615913391\n",
      "Epoch 153/300, Loss: 0.05436239391565323\n",
      "Epoch 154/300, Loss: 0.05395665764808655\n",
      "Epoch 155/300, Loss: 0.0521719790995121\n",
      "Epoch 156/300, Loss: 0.05139180272817612\n",
      "Epoch 157/300, Loss: 0.054332152009010315\n",
      "Epoch 158/300, Loss: 0.0531756654381752\n",
      "Epoch 159/300, Loss: 0.055325623601675034\n",
      "Epoch 160/300, Loss: 0.054125748574733734\n",
      "Epoch 161/300, Loss: 0.05276261642575264\n",
      "Epoch 162/300, Loss: 0.05390750244259834\n",
      "Epoch 163/300, Loss: 0.05368884652853012\n",
      "Epoch 164/300, Loss: 0.055739302188158035\n",
      "Epoch 165/300, Loss: 0.05264665186405182\n",
      "Epoch 166/300, Loss: 0.05441910773515701\n",
      "Epoch 167/300, Loss: 0.053507495671510696\n",
      "Epoch 168/300, Loss: 0.052760373800992966\n",
      "Epoch 169/300, Loss: 0.051772862672805786\n",
      "Epoch 170/300, Loss: 0.05113426595926285\n",
      "Epoch 171/300, Loss: 0.05261630192399025\n",
      "Epoch 172/300, Loss: 0.051630839705467224\n",
      "Epoch 173/300, Loss: 0.050665441900491714\n",
      "Epoch 174/300, Loss: 0.05360886827111244\n",
      "Epoch 175/300, Loss: 0.05115118250250816\n",
      "Epoch 176/300, Loss: 0.05154950171709061\n",
      "Epoch 177/300, Loss: 0.05179725959897041\n",
      "Epoch 178/300, Loss: 0.0526236928999424\n",
      "Epoch 179/300, Loss: 0.05151566118001938\n",
      "Epoch 180/300, Loss: 0.05205164849758148\n",
      "Epoch 181/300, Loss: 0.05086814612150192\n",
      "Epoch 182/300, Loss: 0.051708951592445374\n",
      "Epoch 183/300, Loss: 0.05206648260354996\n",
      "Epoch 184/300, Loss: 0.05216456204652786\n",
      "Epoch 185/300, Loss: 0.05123881995677948\n",
      "Epoch 186/300, Loss: 0.04960788041353226\n",
      "Epoch 187/300, Loss: 0.050715360790491104\n",
      "Epoch 188/300, Loss: 0.05040987581014633\n",
      "Epoch 189/300, Loss: 0.04927030950784683\n",
      "Epoch 190/300, Loss: 0.05187258496880531\n",
      "Epoch 191/300, Loss: 0.05085810646414757\n",
      "Epoch 192/300, Loss: 0.052172496914863586\n",
      "Epoch 193/300, Loss: 0.049383461475372314\n",
      "Epoch 194/300, Loss: 0.05048277974128723\n",
      "Epoch 195/300, Loss: 0.051602303981781006\n",
      "Epoch 196/300, Loss: 0.049792975187301636\n",
      "Epoch 197/300, Loss: 0.049570564180612564\n",
      "Epoch 198/300, Loss: 0.049426332116127014\n",
      "Epoch 199/300, Loss: 0.05004873126745224\n",
      "Epoch 200/300, Loss: 0.04957222566008568\n",
      "Epoch 201/300, Loss: 0.049472562968730927\n",
      "Epoch 202/300, Loss: 0.048199497163295746\n",
      "Epoch 203/300, Loss: 0.04975301772356033\n",
      "Epoch 204/300, Loss: 0.05036022886633873\n",
      "Epoch 205/300, Loss: 0.04994065687060356\n",
      "Epoch 206/300, Loss: 0.05110049247741699\n",
      "Epoch 207/300, Loss: 0.04920755699276924\n",
      "Epoch 208/300, Loss: 0.05139390751719475\n",
      "Epoch 209/300, Loss: 0.04938606917858124\n",
      "Epoch 210/300, Loss: 0.050095539540052414\n",
      "Epoch 211/300, Loss: 0.048510126769542694\n",
      "Epoch 212/300, Loss: 0.050024427473545074\n",
      "Epoch 213/300, Loss: 0.04899616912007332\n",
      "Epoch 214/300, Loss: 0.048174865543842316\n",
      "Epoch 215/300, Loss: 0.05243762210011482\n",
      "Epoch 216/300, Loss: 0.04830697551369667\n",
      "Epoch 217/300, Loss: 0.04890058934688568\n",
      "Epoch 218/300, Loss: 0.04882284998893738\n",
      "Epoch 219/300, Loss: 0.049406103789806366\n",
      "Epoch 220/300, Loss: 0.04880443960428238\n",
      "Epoch 221/300, Loss: 0.0496760793030262\n",
      "Epoch 222/300, Loss: 0.049069538712501526\n",
      "Epoch 223/300, Loss: 0.04743960499763489\n",
      "Epoch 224/300, Loss: 0.04865071177482605\n",
      "Epoch 225/300, Loss: 0.046931713819503784\n",
      "Epoch 226/300, Loss: 0.048610471189022064\n",
      "Epoch 227/300, Loss: 0.04836452007293701\n",
      "Epoch 228/300, Loss: 0.05016506463289261\n",
      "Epoch 229/300, Loss: 0.0473371297121048\n",
      "Epoch 230/300, Loss: 0.04781275615096092\n",
      "Epoch 231/300, Loss: 0.04797622561454773\n",
      "Epoch 232/300, Loss: 0.04688633605837822\n",
      "Epoch 233/300, Loss: 0.04650404676795006\n",
      "Epoch 234/300, Loss: 0.046768009662628174\n",
      "Epoch 235/300, Loss: 0.04731126129627228\n",
      "Epoch 236/300, Loss: 0.046188827604055405\n",
      "Epoch 237/300, Loss: 0.04905840381979942\n",
      "Epoch 238/300, Loss: 0.04806036129593849\n",
      "Epoch 239/300, Loss: 0.04954150319099426\n",
      "Epoch 240/300, Loss: 0.04976268485188484\n",
      "Epoch 241/300, Loss: 0.04796287789940834\n",
      "Epoch 242/300, Loss: 0.04781607910990715\n",
      "Epoch 243/300, Loss: 0.0482889823615551\n",
      "Epoch 244/300, Loss: 0.04854878783226013\n",
      "Epoch 245/300, Loss: 0.04867551848292351\n",
      "Epoch 246/300, Loss: 0.047684527933597565\n",
      "Epoch 247/300, Loss: 0.046834547072649\n",
      "Epoch 248/300, Loss: 0.04878474399447441\n",
      "Epoch 249/300, Loss: 0.04719196632504463\n",
      "Epoch 250/300, Loss: 0.04553142935037613\n",
      "Epoch 251/300, Loss: 0.046986620873212814\n",
      "Epoch 252/300, Loss: 0.049427472054958344\n",
      "Epoch 253/300, Loss: 0.04872670769691467\n",
      "Epoch 254/300, Loss: 0.04878273606300354\n",
      "Epoch 255/300, Loss: 0.04933756962418556\n",
      "Epoch 256/300, Loss: 0.04744689539074898\n",
      "Epoch 257/300, Loss: 0.048141032457351685\n",
      "Epoch 258/300, Loss: 0.04967706277966499\n",
      "Epoch 259/300, Loss: 0.049866244196891785\n",
      "Epoch 260/300, Loss: 0.04817371442914009\n",
      "Epoch 261/300, Loss: 0.04871411621570587\n",
      "Epoch 262/300, Loss: 0.048968151211738586\n",
      "Epoch 263/300, Loss: 0.04579654335975647\n",
      "Epoch 264/300, Loss: 0.04755382612347603\n",
      "Epoch 265/300, Loss: 0.04646550863981247\n",
      "Epoch 266/300, Loss: 0.047749679535627365\n",
      "Epoch 267/300, Loss: 0.04742779582738876\n",
      "Epoch 268/300, Loss: 0.046895820647478104\n",
      "Epoch 269/300, Loss: 0.045431189239025116\n",
      "Epoch 270/300, Loss: 0.04591860622167587\n",
      "Epoch 271/300, Loss: 0.047313518822193146\n",
      "Epoch 272/300, Loss: 0.04727323353290558\n",
      "Epoch 273/300, Loss: 0.044994402676820755\n",
      "Epoch 274/300, Loss: 0.04585859924554825\n",
      "Epoch 275/300, Loss: 0.04741792008280754\n",
      "Epoch 276/300, Loss: 0.046406056731939316\n",
      "Epoch 277/300, Loss: 0.04538685083389282\n",
      "Epoch 278/300, Loss: 0.04828133061528206\n",
      "Epoch 279/300, Loss: 0.04427681863307953\n",
      "Epoch 280/300, Loss: 0.04554801806807518\n",
      "Epoch 281/300, Loss: 0.046606093645095825\n",
      "Epoch 282/300, Loss: 0.045981988310813904\n",
      "Epoch 283/300, Loss: 0.04505276679992676\n",
      "Epoch 284/300, Loss: 0.045788269490003586\n",
      "Epoch 285/300, Loss: 0.04634469002485275\n",
      "Epoch 286/300, Loss: 0.04653029516339302\n",
      "Epoch 287/300, Loss: 0.047796882688999176\n",
      "Epoch 288/300, Loss: 0.04853394255042076\n",
      "Epoch 289/300, Loss: 0.04685842618346214\n",
      "Epoch 290/300, Loss: 0.04723016172647476\n",
      "Epoch 291/300, Loss: 0.045837584882974625\n",
      "Epoch 292/300, Loss: 0.04533375799655914\n",
      "Epoch 293/300, Loss: 0.046470675617456436\n",
      "Epoch 294/300, Loss: 0.045849353075027466\n",
      "Epoch 295/300, Loss: 0.04847942292690277\n",
      "Epoch 296/300, Loss: 0.045059654861688614\n",
      "Epoch 297/300, Loss: 0.043981414288282394\n",
      "Epoch 298/300, Loss: 0.04474326968193054\n",
      "Epoch 299/300, Loss: 0.045917361974716187\n",
      "Epoch 300/300, Loss: 0.04496804624795914\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T13:11:36.813076Z",
     "start_time": "2025-02-23T13:11:36.782437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 8. Evaluate the Model ---\n",
    "\n",
    "val_loss = 0\n",
    "for i in range(0, X_val.shape[0], batch_size):\n",
    "    X_batch = X_val[i:i + batch_size]\n",
    "    y_batch = y_val[i:i + batch_size]\n",
    "    random_target_batch = generate_random_target(X_batch.shape[0])\n",
    "\n",
    "    predictions = lstm_model([X_batch, random_target_batch], training=False)\n",
    "    loss = custom_loss(y_batch, predictions, random_target_batch)\n",
    "    val_loss += loss\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / (X_val.shape[0] // batch_size)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.11377500742673874\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T13:22:16.368497Z",
     "start_time": "2025-02-23T13:22:16.327272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 9. Make Predictions ---\n",
    "\n",
    "# Generate a random seed for prediction\n",
    "prediction_random_seed = generate_random_target(1)\n",
    "\n",
    "# Get the last sequence from X_val and reshape it for prediction\n",
    "last_sequence = X[-1].reshape(1, seq_length, all_cols)\n",
    "\n",
    "# Make a prediction using the last sequence and the random seed\n",
    "prediction = lstm_model.predict([last_sequence, prediction_random_seed])\n",
    "\n",
    "# Inverse transform to get predictions in the original scale\n",
    "prediction = scaler.inverse_transform(np.concatenate((prediction, np.zeros((prediction.shape[0], all_cols - pred_cols))), axis=1))[:, :pred_cols]\n",
    "\n",
    "prediction"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.22921571,  6.27794999, 17.12253898, 17.51735818, 19.63531116,\n",
       "        14.97618437, 36.4119612 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T18:01:18.559036Z",
     "start_time": "2025-02-07T18:01:18.554775Z"
    }
   },
   "cell_type": "code",
   "source": "print(scaler.inverse_transform(X[-1])[:, :pred_cols]);",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  7. 22. 30. 41. 44. 49.]\n",
      " [ 1.  9.  9. 37. 41. 42. 49.]\n",
      " [ 3.  8.  2. 11. 19. 30. 49.]\n",
      " [ 4.  5.  6.  8. 14. 27. 41.]\n",
      " [ 6.  8.  8. 15. 24. 35. 42.]\n",
      " [ 5.  9. 18. 20. 29. 41. 48.]\n",
      " [ 6.  7. 12. 27. 36. 37. 42.]]\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
